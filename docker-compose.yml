services:
  db:
    image: postgres:17-alpine
    restart: unless-stopped
    environment:
      # PostgreSQL uses POSTGRES_* for the superuser that initializes the DB
      POSTGRES_USER: ${DB_ADMIN_USER}
      POSTGRES_PASSWORD: ${DB_ADMIN_PASSWORD}
      POSTGRES_DB: ${DB_DATABASE}
      # Pass these for init.sql to create the API worker user
      DB_API_WORKER_USER: ${DB_API_WORKER_USER}
      DB_API_WORKER_PASSWORD: ${DB_API_WORKER_PASSWORD}
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./db/init.sql:/docker-entrypoint-initdb.d/10-init.sql:ro
      - ./db/init-users.sh:/docker-entrypoint-initdb.d/20-init-users.sh:ro
    ports:
      - "${DB_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - internal

  cache:
    image: redis:7.2-alpine
    restart: unless-stopped
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  mqtt:
    build:
      context: .
      dockerfile: mqtt/Dockerfile
    restart: unless-stopped
    env_file:
      - .env
    environment:
      BACKEND_PASSWORD: ${MQTT_PASSWORD:-}
    volumes:
      - ./mqtt/certs:/mosquitto/certs
      - ./mqtt/config:/mosquitto/config
    ports:
      - "1883:1883"
      - "8883:8883"
      - "9001:9001"
    healthcheck:
      test: ["CMD-SHELL", "netstat -tlnp | grep -q ':1883' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      internal: {}
      caddy_net:
        aliases:
          - mqtt

  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    restart: unless-stopped
    working_dir: /workspace/api
    env_file:
      - .env
    environment:
      PYTHONPATH: /workspace/api
      CELERY_BROKER_URL: ${CELERY_BROKER_URL:-redis://cache:6379/0}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND:-redis://cache:6379/0}
      MQTT_CA_FILE: ${MQTT_CA_FILE:-/certs/ca.crt}
      MQTT_CA_KEY_FILE: ${MQTT_CA_KEY_FILE:-/certs/ca.key}
      MQTT_PASSWD_FILE: ${MQTT_PASSWD_FILE:-/mqtt-config/passwords}
      MQTT_BROKER_HOST: ${MQTT_BROKER_HOST:-mqtt}
      MQTT_BROKER_PORT: ${MQTT_BROKER_PORT:-1883}
      MQTT_TLS_ENABLED: ${MQTT_TLS_ENABLED:-false}
      MQTT_PUBLIC_HOST: ${MQTT_PUBLIC_HOST:-dev.makapix.club}
      MQTT_PUBLIC_PORT: ${MQTT_PUBLIC_PORT:-8883}
      VAULT_LOCATION: ${VAULT_LOCATION}
      MAKAPIX_ADMIN_USER: ${MAKAPIX_ADMIN_USER}
      MAKAPIX_ADMIN_PASSWORD: ${MAKAPIX_ADMIN_PASSWORD}
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./api:/workspace/api
      - api_pip_cache:/root/.cache/pip
      - ./mqtt/certs:/certs:ro
      - ./mqtt/config:/mqtt-config
      - ${VAULT_HOST_PATH}:${VAULT_LOCATION}
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      db:
        condition: service_healthy
      cache:
        condition: service_healthy
      mqtt:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      internal: {}
      proxy: {}
      caddy_net:
        aliases:
          - api

  api-test:
    profiles: ["test"]
    build:
      context: .
      dockerfile: api/Dockerfile
    working_dir: /workspace/api
    env_file:
      - .env
    environment:
      PYTHONPATH: /workspace/api
      CELERY_BROKER_URL: ${CELERY_BROKER_URL:-redis://cache:6379/0}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND:-redis://cache:6379/0}
      MQTT_CA_FILE: ${MQTT_CA_FILE:-/certs/ca.crt}
    command: pytest -q
    volumes:
      - ./api:/workspace/api
      - api_pip_cache:/root/.cache/pip
      - ./mqtt/certs:/certs:ro
    depends_on:
      db:
        condition: service_healthy
      cache:
        condition: service_healthy
    networks:
      - internal

  worker:
    build:
      context: .
      dockerfile: worker/Dockerfile
    restart: unless-stopped
    env_file:
      - .env
    environment:
      PYTHONPATH: /workspace/api
      CELERY_BROKER_URL: ${CELERY_BROKER_URL:-redis://cache:6379/0}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND:-redis://cache:6379/0}
      MQTT_CA_FILE: ${MQTT_CA_FILE:-/certs/ca.crt}
      VAULT_LOCATION: ${VAULT_LOCATION}
    command: python /workspace/worker/worker.py
    volumes:
      - ./api:/workspace/api
      - ./worker:/workspace/worker
      - api_pip_cache:/root/.cache/pip
      - ./mqtt/certs:/certs:ro
      - ${VAULT_HOST_PATH}:${VAULT_LOCATION}
    depends_on:
      cache:
        condition: service_healthy
      db:
        condition: service_healthy
    networks:
      - internal

  # ---------------------------------------------------------------------------
  # Web Application (Next.js) - LOCAL DEVELOPMENT ONLY
  # ---------------------------------------------------------------------------
  # ⚠️  WARNING: This service is for LOCAL DEVELOPMENT with hot reload.
  #     DO NOT run this on the VPS when deploy/stack is active!
  #
  #     For VPS deployment, use: cd deploy/stack && docker compose up -d web
  #     That uses production mode which avoids mobile browser issues.
  #
  #     If you see "makapix-web-1" running on the VPS, stop it:
  #       docker stop makapix-web-1 && docker rm makapix-web-1
  # ---------------------------------------------------------------------------
  web:
    build:
      context: .
      dockerfile: web/Dockerfile
    restart: unless-stopped
    env_file:
      - .env
    environment:
      HOST: 0.0.0.0
      PORT: 3000
      NEXT_TELEMETRY_DISABLED: "1"
      NEXT_PUBLIC_API_BASE_URL: ${NEXT_PUBLIC_API_BASE_URL:-http://localhost}
      API_INTERNAL_URL: http://api:8000
      NEXT_PUBLIC_MQTT_WS_URL: ${NEXT_PUBLIC_MQTT_WS_URL:-wss://makapix.club:9001}
    labels:
      # Expose dev site via the host-level caddy-docker-proxy (container name: caddy)
      caddy: dev.makapix.club
      caddy.header.X-Robots-Tag: "noindex, nofollow, noarchive"
      # Route API calls to the backend service
      caddy.handle_path: /api/*
      caddy.handle_path.reverse_proxy: api:8000
      # Everything else goes to Next.js
      caddy.reverse_proxy: web:3000
    # Production mode: use pre-built Docker image (no volume mounts, no npm install)
    # For local dev with hot reload, use docker-compose.override.local.yml
    # Keep Next.js static assets across deploys so older clients don't 404 on hashed chunks.
    volumes:
      - web_next_static:/app/.next/static
    ports:
      - "${WEB_PORT:-3000}:3000"
    depends_on:
      api:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3000 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 35s
    networks:
      internal: {}
      proxy: {}
      caddy_net:
        aliases:
          - web

  proxy:
    image: caddy:2
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./proxy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      api:
        condition: service_healthy
      web:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost/proxy-healthz || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - proxy
      - internal

networks:
  internal:
    driver: bridge
  proxy:
    driver: bridge
  caddy_net:
    external: true

volumes:
  pg_data:
  api_pip_cache:
  web_next_static:
  caddy_data:
  caddy_config:
